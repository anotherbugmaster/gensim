{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim Tutorial on Online Non-Negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooks explains basic ideas behind the open source NMF implementation in [Gensim](https://github.com/RaRe-Technologies/gensim), including code examples for applying NMF to text processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's in this tutorial?\n",
    "\n",
    "1. [Introduction: Why NMF?](#1.-Introduction-to-NMF)\n",
    "2. [Code example on 20 Newsgroups](#2.-Code-example:-NMF-on-20-Newsgroups)\n",
    "3. [Benchmarks against Sklearn's NMF and Gensim's LDA](#3.-Benchmarks)\n",
    "4. [Large-scale NMF training on the English Wikipedia (sparse text vectors)](#4.-NMF-on-English-Wikipedia)\n",
    "5. [NMF on face decomposition (dense image vectors)](#5.-And-now-for-something-completely-different:-Face-decomposition-from-images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# 1. Introduction to NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## What's in a name?\n",
    "\n",
    "Gensim's Online Non-Negative Matrix Factorization (NMF, NNMF, ONMF) implementation is based on [Renbo Zhao, Vincent Y. F. Tan: Online Nonnegative Matrix Factorization with Outliers, 2016](https://arxiv.org/abs/1604.02634) and is optimized for extremely large, sparse, streamed inputs. Such inputs happen in NLP with **unsupervised training** on massive text corpora.\n",
    "\n",
    "* Why **Online**? Because corpora and datasets in modern ML can be very large, and RAM is limited. Unlike batch algorithms, online algorithms learn iteratively, streaming through the available training examples, without loading the entire dataset into RAM or requiring random-access to the data examples.\n",
    "\n",
    "* Why **Non-Negative**? Because non-negativity leads to more interpretable, sparse \"human-friendly\" topics. This is in contrast to e.g. SVD (another popular matrix factorization method with [super-efficient implementation in Gensim](https://radimrehurek.com/gensim/models/lsimodel.html)), which produces dense negative factors and thus harder-to-interpret topics.\n",
    "\n",
    "* **Matrix factorizations** are the corner stone of modern machine learning. They can be used either directly (recommendation systems, bi-clustering, image compression, topic modelingâ€¦) or as internal routines in more complex deep learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## How ONNMF works\n",
    "\n",
    "Terminology:\n",
    "- `corpus` is a stream of input documents = training examples\n",
    "- `batch` is a chunk of input corpus, a word-document matrix mini-batch that fits in RAM\n",
    "- `W` is a word-topic matrix (to be learned; stored in the resulting model)\n",
    "- `h` is a topic-document matrix (to be learned; not stored, but rather inferred for documents on-the-fly)\n",
    "- `A`, `B` - matrices that accumulate information from consecutive chunks. `A = h.dot(ht)`, `B = v.dot(ht)`.\n",
    "\n",
    "The idea behind the algorithm is as follows:\n",
    "\n",
    "```\n",
    "    Initialize W, A and B matrices\n",
    "\n",
    "    for batch in input corpus batches:\n",
    "        infer h:\n",
    "            do coordinate gradient descent step to find h that minimizes ||batch - Wh|| in L2 norm\n",
    "\n",
    "            bound h so that it is non-negative\n",
    "\n",
    "        update A and B:\n",
    "            A = h.dot(ht)\n",
    "            B = batch.dot(ht)\n",
    "\n",
    "        update W:\n",
    "            do gradient descent step to find W that minimizes ||0.5*trace(WtWA) - trace(WtB)|| in L2 norm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Code example: NMF on 20 Newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the models we'll be using throughout this tutorial (`numpy==1.14.2`, `matplotlib==3.0.2`, `pandas==0.24.1`, `sklearn==0.19.1`, `gensim==3.7.1`) and set up logging at INFO level.\n",
    "\n",
    "Gensim uses logging generously to inform users what's going on. Eyeballing the logs is a good sanity check, to make sure everything is working as expected.\n",
    "\n",
    "Only `numpy` and `gensim` are actually needed to train and use NMF. The other imports are used only to make our life a little easier in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import os\n",
    "from multiprocessing import Process\n",
    "import psutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.random import RandomState\n",
    "from sklearn import decomposition\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.decomposition.nmf import NMF as SklearnNmf\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import gensim.downloader\n",
    "from gensim import matutils, utils\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel, LdaModel, TfidfModel\n",
    "from gensim.models.basemodel import BaseTopicModel\n",
    "from gensim.models.nmf import Nmf as GensimNmf\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the notorious [20 Newsgroups dataset](http://qwone.com/~jason/20Newsgroups/) from Gensim's [repository of pre-trained models and corpora](https://github.com/RaRe-Technologies/gensim-data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups = gensim.downloader.load('20-newsgroups')\n",
    "\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'comp.graphics',\n",
    "    'rec.motorcycles',\n",
    "    'talk.politics.mideast',\n",
    "    'sci.space'\n",
    "]\n",
    "\n",
    "categories = {name: idx for idx, name in enumerate(categories)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a train/test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = RandomState(42)\n",
    "\n",
    "trainset = np.array([\n",
    "    {\n",
    "        'data': doc['data'],\n",
    "        'target': categories[doc['topic']],\n",
    "    }\n",
    "    for doc in newsgroups\n",
    "    if doc['topic'] in categories and doc['set'] == 'train'\n",
    "])\n",
    "random_state.shuffle(trainset)\n",
    "\n",
    "testset = np.array([\n",
    "    {\n",
    "        'data': doc['data'],\n",
    "        'target': categories[doc['topic']],\n",
    "    }\n",
    "    for doc in newsgroups\n",
    "    if doc['topic'] in categories and doc['set'] == 'test'\n",
    "])\n",
    "random_state.shuffle(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use very [simple preprocessing with stemming](https://radimrehurek.com/gensim/parsing/preprocessing.html#gensim.parsing.preprocessing.preprocess_string) to tokenize each document. YMMV; in your application, use whatever preprocessing makes sense in your domain. Correctly preparing the input has [major impact](https://en.wikipedia.org/wiki/Garbage_in,_garbage_out) on any subsequent ML training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_documents = [preprocess_string(doc['data']) for doc in trainset]\n",
    "test_documents = [preprocess_string(doc['data']) for doc in testset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a mapping between tokens and their ids. Another option would be a [HashDictionary](https://radimrehurek.com/gensim/corpora/hashdictionary.html), saving ourselves one pass over the training documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-04 01:54:20,844 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-03-04 01:54:21,153 : INFO : built Dictionary(25279 unique tokens: ['gladli', 'garrett', 'stuck', 'gov', 'karasi']...) from 2819 documents (total 435328 corpus positions)\n",
      "2019-03-04 01:54:21,182 : INFO : discarding 18198 tokens: [('batka', 1), ('batkaj', 1), ('beatl', 1), ('ccmail', 3), ('dayton', 4), ('edu', 1785), ('inhibit', 1), ('jbatka', 1), ('line', 2748), ('organ', 2602)]...\n",
      "2019-03-04 01:54:21,183 : INFO : keeping 7081 tokens which were in no less than 5 and no more than 1409 (=50.0%) documents\n",
      "2019-03-04 01:54:21,193 : INFO : resulting dictionary: Dictionary(7081 unique tokens: ['gladli', 'run', 'trillion', 'stuck', 'order']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = Dictionary(train_documents)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=20000)  # filter out too in/frequent tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's vectorize the training corpus into the bag-of-words format. We'll train LDA on a BOW and NMFs on an TF-IDF corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfModel(dictionary=dictionary)\n",
    "\n",
    "train_corpus = [\n",
    "    dictionary.doc2bow(document)\n",
    "    for document\n",
    "    in train_documents\n",
    "]\n",
    "\n",
    "test_corpus = [\n",
    "    dictionary.doc2bow(document)\n",
    "    for document\n",
    "    in test_documents\n",
    "]\n",
    "\n",
    "train_corpus_tfidf = list(tfidf[train_corpus])\n",
    "\n",
    "test_corpus_tfidf = list(tfidf[test_corpus])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we simply stored the bag-of-words vectors into a `list`, but Gensim accepts [any iterable](https://radimrehurek.com/gensim/tut1.html#corpus-streaming-one-document-at-a-time) as input, including streamed ones. To learn more about memory-efficient input iterables, see our [Data Streaming in Python: Generators, Iterators, Iterables](https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/) tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF Model Training\n",
    "\n",
    "The API works in the same way as other Gensim models, such as [LdaModel](https://radimrehurek.com/gensim/models/ldamodel.html) or [LsiModel](https://radimrehurek.com/gensim/models/lsimodel.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notable model parameters:\n",
    "\n",
    "- `kappa` float, optional\n",
    "\n",
    "    Gradient descent step size.\n",
    "    Larger value makes the model train faster, but could lead to non-convergence if set too large.\n",
    "    \n",
    "- `w_max_iter` int, optional\n",
    "\n",
    "    Maximum number of iterations to train W per each batch.\n",
    "    \n",
    "- `w_stop_condition` float, optional\n",
    "\n",
    "    If the error difference gets smaller than this, training of ``W`` stops for the current batch.\n",
    "    \n",
    "- `h_r_max_iter` int, optional\n",
    "\n",
    "    Maximum number of iterations to train h per each batch.\n",
    "    \n",
    "- `h_r_stop_condition` float, optional\n",
    "\n",
    "    If the error difference gets smaller than this, training of ``h`` stops for the current batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn an NMF model with 5 topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-04 01:54:24,559 : INFO : running NMF training, 5 topics, 5 passes over the supplied corpus of 2819 documents, evaluating l2 norm every 2819 documents\n",
      "2019-03-04 01:54:24,574 : INFO : PROGRESS: pass 0, at document #1000/2819\n",
      "2019-03-04 01:54:24,589 : INFO : W error diff: -inf\n",
      "2019-03-04 01:54:24,604 : INFO : PROGRESS: pass 0, at document #2000/2819\n",
      "2019-03-04 01:54:24,615 : INFO : W error diff: -2.1783593508632997\n",
      "2019-03-04 01:54:24,626 : INFO : PROGRESS: pass 0, at document #2819/2819\n",
      "2019-03-04 01:54:24,722 : INFO : L2 norm: 28.137070033533682\n",
      "2019-03-04 01:54:24,756 : INFO : topic #0 (0.404): 0.011*\"isra\" + 0.010*\"israel\" + 0.007*\"arab\" + 0.006*\"jew\" + 0.005*\"palestinian\" + 0.004*\"henri\" + 0.003*\"toronto\" + 0.003*\"question\" + 0.003*\"kill\" + 0.003*\"polici\"\n",
      "2019-03-04 01:54:24,757 : INFO : topic #1 (0.358): 0.009*\"space\" + 0.005*\"access\" + 0.005*\"nasa\" + 0.004*\"pat\" + 0.003*\"digex\" + 0.003*\"orbit\" + 0.003*\"shuttl\" + 0.003*\"graphic\" + 0.003*\"data\" + 0.003*\"com\"\n",
      "2019-03-04 01:54:24,757 : INFO : topic #2 (0.388): 0.013*\"armenian\" + 0.006*\"turkish\" + 0.005*\"greek\" + 0.005*\"peopl\" + 0.004*\"armenia\" + 0.004*\"turk\" + 0.004*\"argic\" + 0.004*\"bike\" + 0.003*\"serdar\" + 0.003*\"turkei\"\n",
      "2019-03-04 01:54:24,758 : INFO : topic #3 (0.423): 0.010*\"moral\" + 0.006*\"keith\" + 0.004*\"anim\" + 0.004*\"jake\" + 0.003*\"boni\" + 0.003*\"act\" + 0.003*\"instinct\" + 0.003*\"think\" + 0.003*\"caltech\" + 0.003*\"object\"\n",
      "2019-03-04 01:54:24,759 : INFO : topic #4 (0.441): 0.009*\"islam\" + 0.009*\"god\" + 0.006*\"muslim\" + 0.006*\"livesei\" + 0.005*\"imag\" + 0.005*\"sgi\" + 0.005*\"jaeger\" + 0.004*\"jon\" + 0.004*\"solntz\" + 0.004*\"wpd\"\n",
      "2019-03-04 01:54:24,765 : INFO : W error diff: -0.6087333117616911\n",
      "2019-03-04 01:54:24,779 : INFO : PROGRESS: pass 1, at document #1000/2819\n",
      "2019-03-04 01:54:24,787 : INFO : W error diff: -1.5858439279007879\n",
      "2019-03-04 01:54:24,801 : INFO : PROGRESS: pass 1, at document #2000/2819\n",
      "2019-03-04 01:54:24,807 : INFO : W error diff: -1.1329837530094071\n",
      "2019-03-04 01:54:24,820 : INFO : PROGRESS: pass 1, at document #2819/2819\n",
      "2019-03-04 01:54:24,914 : INFO : L2 norm: 28.02006726219276\n",
      "2019-03-04 01:54:24,947 : INFO : topic #0 (0.345): 0.014*\"israel\" + 0.014*\"isra\" + 0.009*\"arab\" + 0.007*\"jew\" + 0.005*\"palestinian\" + 0.004*\"lebanes\" + 0.004*\"peac\" + 0.003*\"polici\" + 0.003*\"attack\" + 0.003*\"henri\"\n",
      "2019-03-04 01:54:24,947 : INFO : topic #1 (0.253): 0.008*\"space\" + 0.005*\"nasa\" + 0.004*\"access\" + 0.003*\"orbit\" + 0.003*\"pat\" + 0.003*\"digex\" + 0.003*\"launch\" + 0.003*\"shuttl\" + 0.003*\"graphic\" + 0.003*\"com\"\n",
      "2019-03-04 01:54:24,948 : INFO : topic #2 (0.299): 0.020*\"armenian\" + 0.010*\"turkish\" + 0.007*\"armenia\" + 0.006*\"turk\" + 0.006*\"argic\" + 0.006*\"serdar\" + 0.005*\"greek\" + 0.005*\"turkei\" + 0.004*\"genocid\" + 0.004*\"peopl\"\n",
      "2019-03-04 01:54:24,949 : INFO : topic #3 (0.353): 0.013*\"moral\" + 0.011*\"keith\" + 0.006*\"object\" + 0.005*\"caltech\" + 0.005*\"schneider\" + 0.004*\"anim\" + 0.004*\"allan\" + 0.004*\"cco\" + 0.004*\"jake\" + 0.004*\"boni\"\n",
      "2019-03-04 01:54:24,949 : INFO : topic #4 (0.380): 0.011*\"islam\" + 0.011*\"god\" + 0.006*\"livesei\" + 0.006*\"sgi\" + 0.006*\"jaeger\" + 0.005*\"muslim\" + 0.005*\"jon\" + 0.005*\"religion\" + 0.004*\"imag\" + 0.004*\"solntz\"\n",
      "2019-03-04 01:54:24,953 : INFO : W error diff: -0.05304441334403265\n",
      "2019-03-04 01:54:24,967 : INFO : PROGRESS: pass 2, at document #1000/2819\n",
      "2019-03-04 01:54:24,973 : INFO : W error diff: -0.6532464912217009\n",
      "2019-03-04 01:54:24,988 : INFO : PROGRESS: pass 2, at document #2000/2819\n",
      "2019-03-04 01:54:24,993 : INFO : W error diff: -0.5542774416923812\n",
      "2019-03-04 01:54:25,005 : INFO : PROGRESS: pass 2, at document #2819/2819\n",
      "2019-03-04 01:54:25,099 : INFO : L2 norm: 27.999892226543682\n",
      "2019-03-04 01:54:25,132 : INFO : topic #0 (0.343): 0.014*\"israel\" + 0.014*\"isra\" + 0.009*\"arab\" + 0.008*\"jew\" + 0.005*\"palestinian\" + 0.004*\"lebanes\" + 0.004*\"peac\" + 0.003*\"attack\" + 0.003*\"polici\" + 0.003*\"lebanon\"\n",
      "2019-03-04 01:54:25,133 : INFO : topic #1 (0.229): 0.007*\"space\" + 0.005*\"nasa\" + 0.004*\"access\" + 0.003*\"orbit\" + 0.003*\"pat\" + 0.003*\"launch\" + 0.003*\"digex\" + 0.003*\"gov\" + 0.003*\"graphic\" + 0.003*\"com\"\n",
      "2019-03-04 01:54:25,134 : INFO : topic #2 (0.283): 0.022*\"armenian\" + 0.011*\"turkish\" + 0.007*\"armenia\" + 0.007*\"turk\" + 0.007*\"argic\" + 0.007*\"serdar\" + 0.006*\"turkei\" + 0.005*\"greek\" + 0.005*\"genocid\" + 0.004*\"soviet\"\n",
      "2019-03-04 01:54:25,134 : INFO : topic #3 (0.347): 0.015*\"moral\" + 0.013*\"keith\" + 0.007*\"object\" + 0.006*\"caltech\" + 0.005*\"schneider\" + 0.005*\"allan\" + 0.005*\"cco\" + 0.004*\"anim\" + 0.004*\"jake\" + 0.004*\"natur\"\n",
      "2019-03-04 01:54:25,135 : INFO : topic #4 (0.365): 0.011*\"god\" + 0.011*\"islam\" + 0.006*\"livesei\" + 0.006*\"sgi\" + 0.006*\"jaeger\" + 0.005*\"muslim\" + 0.005*\"religion\" + 0.005*\"jon\" + 0.005*\"atheist\" + 0.004*\"atheism\"\n",
      "2019-03-04 01:54:25,138 : INFO : W error diff: 0.06399021760879364\n",
      "2019-03-04 01:54:25,151 : INFO : PROGRESS: pass 3, at document #1000/2819\n",
      "2019-03-04 01:54:25,157 : INFO : W error diff: -0.3678424933365889\n",
      "2019-03-04 01:54:25,172 : INFO : PROGRESS: pass 3, at document #2000/2819\n",
      "2019-03-04 01:54:25,177 : INFO : W error diff: -0.34924666183303543\n",
      "2019-03-04 01:54:25,189 : INFO : PROGRESS: pass 3, at document #2819/2819\n",
      "2019-03-04 01:54:25,283 : INFO : L2 norm: 27.991268049236886\n",
      "2019-03-04 01:54:25,315 : INFO : topic #0 (0.350): 0.015*\"israel\" + 0.014*\"isra\" + 0.009*\"arab\" + 0.008*\"jew\" + 0.005*\"palestinian\" + 0.004*\"lebanes\" + 0.004*\"peac\" + 0.003*\"attack\" + 0.003*\"lebanon\" + 0.003*\"polici\"\n",
      "2019-03-04 01:54:25,316 : INFO : topic #1 (0.220): 0.007*\"space\" + 0.005*\"nasa\" + 0.003*\"access\" + 0.003*\"orbit\" + 0.003*\"launch\" + 0.003*\"pat\" + 0.003*\"gov\" + 0.003*\"com\" + 0.003*\"digex\" + 0.002*\"alaska\"\n",
      "2019-03-04 01:54:25,317 : INFO : topic #2 (0.282): 0.023*\"armenian\" + 0.011*\"turkish\" + 0.007*\"armenia\" + 0.007*\"turk\" + 0.007*\"argic\" + 0.007*\"serdar\" + 0.006*\"turkei\" + 0.005*\"greek\" + 0.005*\"genocid\" + 0.005*\"soviet\"\n",
      "2019-03-04 01:54:25,317 : INFO : topic #3 (0.351): 0.016*\"moral\" + 0.015*\"keith\" + 0.007*\"object\" + 0.007*\"caltech\" + 0.006*\"schneider\" + 0.005*\"allan\" + 0.005*\"cco\" + 0.004*\"anim\" + 0.004*\"natur\" + 0.004*\"think\"\n",
      "2019-03-04 01:54:25,318 : INFO : topic #4 (0.364): 0.012*\"god\" + 0.011*\"islam\" + 0.006*\"sgi\" + 0.006*\"jaeger\" + 0.006*\"livesei\" + 0.005*\"muslim\" + 0.005*\"religion\" + 0.005*\"atheist\" + 0.005*\"atheism\" + 0.004*\"jon\"\n",
      "2019-03-04 01:54:25,321 : INFO : W error diff: 0.08877110840856872\n",
      "2019-03-04 01:54:25,334 : INFO : PROGRESS: pass 4, at document #1000/2819\n",
      "2019-03-04 01:54:25,339 : INFO : W error diff: -0.2446709705343757\n",
      "2019-03-04 01:54:25,354 : INFO : PROGRESS: pass 4, at document #2000/2819\n",
      "2019-03-04 01:54:25,359 : INFO : W error diff: -0.24931839405260803\n",
      "2019-03-04 01:54:25,371 : INFO : PROGRESS: pass 4, at document #2819/2819\n",
      "2019-03-04 01:54:25,465 : INFO : L2 norm: 27.98648818098989\n",
      "2019-03-04 01:54:25,498 : INFO : topic #0 (0.354): 0.015*\"israel\" + 0.014*\"isra\" + 0.009*\"arab\" + 0.008*\"jew\" + 0.005*\"palestinian\" + 0.004*\"lebanes\" + 0.004*\"peac\" + 0.004*\"attack\" + 0.003*\"lebanon\" + 0.003*\"polici\"\n",
      "2019-03-04 01:54:25,498 : INFO : topic #1 (0.209): 0.007*\"space\" + 0.005*\"nasa\" + 0.003*\"access\" + 0.003*\"orbit\" + 0.003*\"launch\" + 0.003*\"gov\" + 0.003*\"pat\" + 0.003*\"com\" + 0.002*\"alaska\" + 0.002*\"moon\"\n",
      "2019-03-04 01:54:25,499 : INFO : topic #2 (0.283): 0.023*\"armenian\" + 0.011*\"turkish\" + 0.008*\"armenia\" + 0.007*\"argic\" + 0.007*\"turk\" + 0.007*\"serdar\" + 0.006*\"turkei\" + 0.005*\"greek\" + 0.005*\"genocid\" + 0.005*\"soviet\"\n",
      "2019-03-04 01:54:25,500 : INFO : topic #3 (0.356): 0.017*\"moral\" + 0.016*\"keith\" + 0.007*\"object\" + 0.007*\"caltech\" + 0.006*\"schneider\" + 0.006*\"allan\" + 0.006*\"cco\" + 0.004*\"anim\" + 0.004*\"natur\" + 0.004*\"goal\"\n",
      "2019-03-04 01:54:25,500 : INFO : topic #4 (0.366): 0.012*\"god\" + 0.011*\"islam\" + 0.006*\"jaeger\" + 0.005*\"sgi\" + 0.005*\"livesei\" + 0.005*\"muslim\" + 0.005*\"atheist\" + 0.005*\"religion\" + 0.005*\"atheism\" + 0.004*\"rushdi\"\n",
      "2019-03-04 01:54:25,503 : INFO : W error diff: 0.0932956490045207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.52 s, sys: 1.84 s, total: 3.36 s\n",
      "Wall time: 944 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nmf = GensimNmf(\n",
    "    corpus=train_corpus_tfidf,\n",
    "    num_topics=5,\n",
    "    id2word=dictionary,\n",
    "    chunksize=1000,\n",
    "    passes=5,\n",
    "    eval_every=10,\n",
    "    minimum_probability=0,\n",
    "    random_state=0,\n",
    "    kappa=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the learned topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.015*\"israel\" + 0.014*\"isra\" + 0.009*\"arab\" + 0.008*\"jew\" + 0.005*\"palestinian\" + 0.004*\"lebanes\" + 0.004*\"peac\" + 0.004*\"attack\" + 0.004*\"lebanon\" + 0.003*\"polici\"'),\n",
       " (1,\n",
       "  '0.007*\"space\" + 0.005*\"nasa\" + 0.003*\"access\" + 0.003*\"orbit\" + 0.003*\"launch\" + 0.003*\"gov\" + 0.003*\"pat\" + 0.003*\"com\" + 0.002*\"alaska\" + 0.002*\"moon\"'),\n",
       " (2,\n",
       "  '0.023*\"armenian\" + 0.012*\"turkish\" + 0.008*\"armenia\" + 0.007*\"argic\" + 0.007*\"turk\" + 0.007*\"serdar\" + 0.006*\"turkei\" + 0.005*\"greek\" + 0.005*\"genocid\" + 0.005*\"soviet\"'),\n",
       " (3,\n",
       "  '0.017*\"moral\" + 0.016*\"keith\" + 0.008*\"object\" + 0.007*\"caltech\" + 0.006*\"schneider\" + 0.006*\"allan\" + 0.006*\"cco\" + 0.004*\"anim\" + 0.004*\"natur\" + 0.004*\"goal\"'),\n",
       " (4,\n",
       "  '0.012*\"god\" + 0.011*\"islam\" + 0.006*\"jaeger\" + 0.005*\"sgi\" + 0.005*\"livesei\" + 0.005*\"muslim\" + 0.005*\"atheist\" + 0.005*\"religion\" + 0.005*\"atheism\" + 0.004*\"rushdi\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation measure: Coherence\n",
    "\n",
    "[Topic coherence](http://qpleple.com/topic-coherence-to-evaluate-topic-models/) measures how often do most frequent tokens from each topic co-occur in one document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-04 01:54:25,582 : INFO : CorpusAccumulator accumulated stats from 1000 documents\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-4.045883079644641"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CoherenceModel(\n",
    "    model=nmf,\n",
    "    corpus=test_corpus_tfidf,\n",
    "    coherence='u_mass'\n",
    ").get_coherence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Topic inference on new documents\n",
    "\n",
    "With the NMF model trained, let's fetch one news document not seen during training, and infer its topic vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: spl@ivem.ucsd.edu (Steve Lamont)\n",
      "Subject: Re: RGB to HVS, and back\n",
      "Organization: University of Calif., San Diego/Microscopy and Imaging Resource\n",
      "Lines: 18\n",
      "Distribution: world\n",
      "NNTP-Posting-Host: ivem.ucsd.edu\n",
      "\n",
      "In article <ltu4buINNe7j@caspian.usc.edu> zyeh@caspian.usc.edu (zhenghao yeh) writes:\n",
      ">|> See Foley, van Dam, Feiner, and Hughes, _Computer Graphics: Principles\n",
      ">|> and Practice, Second Edition_.\n",
      ">|> \n",
      ">|> [If people would *read* this book, 75 percent of the questions in this\n",
      ">|> froup would disappear overnight...]\n",
      ">|> \n",
      ">\tNot really. I think it is less than 10%.\n",
      "\n",
      "Nah... I figure most people would be so busy reading that they wouldn't\n",
      "have *time* to post. :-) :-) :-)\n",
      "\n",
      "\t\t\t\t\t\t\tspl\n",
      "-- \n",
      "Steve Lamont, SciViGuy -- (619) 534-7968 -- spl@szechuan.ucsd.edu\n",
      "San Diego Microscopy and Imaging Resource/UC San Diego/La Jolla, CA 92093-0608\n",
      "\"Until I meet you, then, in Upper Hell\n",
      "Convulsed, foaming immortal blood: farewell\" - J. Berryman, \"A Professor's Song\"\n",
      "\n",
      "====================================================================================================\n",
      "Topics: [(0, 0.10094349983895379), (1, 0.40527834628482196), (2, 0.14330724750919113), (3, 0.02887286985628184), (4, 0.32159803651075125)]\n"
     ]
    }
   ],
   "source": [
    "print(testset[0]['data'])\n",
    "print('=' * 100)\n",
    "print(\"Topics: {}\".format(nmf[test_corpus[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word topic inference\n",
    "\n",
    "Similarly, we can inspect the topic distribution assigned to a vocabulary term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: actual\n",
      "Topics: [(0, 0.1517466731147538), (1, 0.2824521057319929), (2, 0.042590027339691805), (3, 0.2520757387076886), (4, 0.2711354551058729)]\n"
     ]
    }
   ],
   "source": [
    "word = dictionary[0]\n",
    "print(\"Word: {}\".format(word))\n",
    "print(\"Topics: {}\".format(nmf.get_term_topics(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal NMF state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Density is a fraction of non-zero elements in a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def density(matrix):\n",
    "    return (matrix > 0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term-topic matrix of shape `(words, topics)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Density: 0.6864567151532269\n"
     ]
    }
   ],
   "source": [
    "print(\"Density: {}\".format(density(nmf._W)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic-document matrix for the last batch of shape `(topics, batch)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Density: 0.662026862026862\n"
     ]
    }
   ],
   "source": [
    "print(\"Density: {}\".format(density(nmf._h)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residuals matrix of the last batch of shape `(words, batch)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim NMF vs Sklearn NMF vs Gensim LDA\n",
    "\n",
    "We'll run these three unsupervised models on the [20newsgroups](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) dataset.\n",
    "\n",
    "20 Newsgroups also contains labels for each document, which will allow us to evaluate the trained models on an \"upstream\" classification task, using the unsupervised document topics as input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "We'll track these metrics as we train and test NMF on the 20-newsgroups corpus we created above:\n",
    "- `train time` - time to train a model\n",
    "- `mean_ram` - mean RAM consumption during training\n",
    "- `max_ram` - maximum RAM consumption during training\n",
    "- `train time` - time to train a model.\n",
    "- `coherence` - coherence score.\n",
    "- `l2_norm` - L2 norm of `v - Wh` (not defined for LDA).\n",
    "- `f1` - [F1 score](https://en.wikipedia.org/wiki/F1_score) on the task of news topic classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "fixed_params = dict(\n",
    "    chunksize=1000,\n",
    "    num_topics=5,\n",
    "    id2word=dictionary,\n",
    "    passes=5,\n",
    "    eval_every=10,\n",
    "    minimum_probability=0,\n",
    "    random_state=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def measure_ram(output, tick=5):\n",
    "    def _measure_ram(pid, output, tick=tick):\n",
    "        py = psutil.Process(pid)\n",
    "        with open(output, 'w') as outfile:\n",
    "            while True:\n",
    "                memory = py.memory_info().rss\n",
    "                outfile.write(\"{}\\n\".format(memory))\n",
    "                outfile.flush()\n",
    "                time.sleep(tick)\n",
    "\n",
    "    pid = os.getpid()\n",
    "    p = Process(target=_measure_ram, args=(pid, output, tick))\n",
    "    p.start()\n",
    "    yield\n",
    "    p.terminate()\n",
    "\n",
    "\n",
    "def get_train_time_and_ram(func, name, tick=5):\n",
    "    memprof_filename = \"{}.memprof\".format(name)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    with measure_ram(memprof_filename, tick=tick):\n",
    "        result = func()\n",
    "\n",
    "    elapsed_time = pd.to_timedelta(time.time() - start, unit='s').round('s')\n",
    "\n",
    "    memprof_df = pd.read_csv(memprof_filename, squeeze=True)\n",
    "\n",
    "    mean_ram = \"{} MB\".format(\n",
    "        int(memprof_df.mean() // 2 ** 20),\n",
    "    )\n",
    "\n",
    "    max_ram = \"{} MB\".format(int(memprof_df.max() // 2 ** 20))\n",
    "\n",
    "    return elapsed_time, mean_ram, max_ram, result\n",
    "\n",
    "\n",
    "def get_f1(model, train_corpus, X_test, y_train, y_test):\n",
    "    if isinstance(model, SklearnNmf):\n",
    "        dense_train_corpus = matutils.corpus2dense(\n",
    "            train_corpus,\n",
    "            num_terms=model.components_.shape[1],\n",
    "        )\n",
    "        X_train = model.transform(dense_train_corpus.T)\n",
    "    else:\n",
    "        X_train = np.zeros((len(train_corpus), model.num_topics))\n",
    "        for bow_id, bow in enumerate(train_corpus):\n",
    "            for topic_id, word_count in model.get_document_topics(bow):\n",
    "                X_train[bow_id, topic_id] = word_count\n",
    "\n",
    "    log_reg = LogisticRegressionCV(multi_class='multinomial', cv=5)\n",
    "    log_reg.fit(X_train, y_train)\n",
    "\n",
    "    pred_labels = log_reg.predict(X_test)\n",
    "\n",
    "    return f1_score(y_test, pred_labels, average='micro')\n",
    "\n",
    "def get_sklearn_topics(model, top_n=5):\n",
    "    topic_probas = model.components_.T\n",
    "    topic_probas = topic_probas / topic_probas.sum(axis=0)\n",
    "\n",
    "    sparsity = np.zeros(topic_probas.shape[1])\n",
    "\n",
    "    for row in topic_probas:\n",
    "        sparsity += (row == 0)\n",
    "\n",
    "    sparsity /= topic_probas.shape[1]\n",
    "\n",
    "    topic_probas = topic_probas[:, sparsity.argsort()[::-1]][:, :top_n]\n",
    "\n",
    "    token_indices = topic_probas.argsort(axis=0)[:-11:-1, :]\n",
    "    topic_probas.sort(axis=0)\n",
    "    topic_probas = topic_probas[:-11:-1, :]\n",
    "\n",
    "    topics = []\n",
    "\n",
    "    for topic_idx in range(topic_probas.shape[1]):\n",
    "        tokens = [\n",
    "            model.id2word[token_idx]\n",
    "            for token_idx\n",
    "            in token_indices[:, topic_idx]\n",
    "        ]\n",
    "        topic = (\n",
    "            '{}*\"{}\"'.format(round(proba, 3), token)\n",
    "            for proba, token\n",
    "            in zip(topic_probas[:, topic_idx], tokens)\n",
    "        )\n",
    "        topic = \" + \".join(topic)\n",
    "        topics.append((topic_idx, topic))\n",
    "\n",
    "    return topics\n",
    "\n",
    "def get_metrics(model, test_corpus, train_corpus=None, y_train=None, y_test=None, dictionary=None):\n",
    "    if isinstance(model, SklearnNmf):\n",
    "        model.get_topics = lambda: model.components_\n",
    "        model.show_topics = lambda top_n: get_sklearn_topics(model, top_n)\n",
    "        model.id2word = dictionary\n",
    "\n",
    "    W = model.get_topics().T\n",
    "\n",
    "    dense_test_corpus = matutils.corpus2dense(\n",
    "        test_corpus,\n",
    "        num_terms=W.shape[0],\n",
    "    )\n",
    "\n",
    "    if isinstance(model, SklearnNmf):\n",
    "        H = model.transform(dense_test_corpus.T).T\n",
    "    else:\n",
    "        H = np.zeros((model.num_topics, len(test_corpus)))\n",
    "        for bow_id, bow in enumerate(test_corpus):\n",
    "            for topic_id, word_count in model.get_document_topics(bow):\n",
    "                H[topic_id, bow_id] = word_count\n",
    "\n",
    "    l2_norm = None\n",
    "\n",
    "    if not isinstance(model, LdaModel):\n",
    "        pred_factors = W.dot(H)\n",
    "\n",
    "        l2_norm = np.linalg.norm(pred_factors - dense_test_corpus)\n",
    "        l2_norm = round(l2_norm, 4)\n",
    "\n",
    "    f1 = None\n",
    "\n",
    "    if train_corpus and y_train and y_test:\n",
    "        f1 = get_f1(model, train_corpus, H.T, y_train, y_test)\n",
    "        f1 = round(f1, 4)\n",
    "\n",
    "    model.normalize = True\n",
    "\n",
    "    coherence = CoherenceModel(\n",
    "        model=model,\n",
    "        corpus=test_corpus,\n",
    "        coherence='u_mass'\n",
    "    ).get_coherence()\n",
    "    coherence = round(coherence, 4)\n",
    "\n",
    "    topics = model.show_topics(5)\n",
    "\n",
    "    model.normalize = False\n",
    "\n",
    "    return dict(\n",
    "        coherence=coherence,\n",
    "        l2_norm=l2_norm,\n",
    "        f1=f1,\n",
    "        topics=topics,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-04 01:54:25,807 : INFO : using symmetric alpha at 0.2\n",
      "2019-03-04 01:54:25,809 : INFO : using symmetric eta at 0.2\n",
      "2019-03-04 01:54:25,811 : INFO : using serial LDA version on this node\n",
      "2019-03-04 01:54:25,817 : INFO : running online (multi-pass) LDA training, 5 topics, 5 passes over the supplied corpus of 2819 documents, updating model once every 1000 documents, evaluating perplexity every 2819 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2019-03-04 01:54:25,818 : INFO : PROGRESS: pass 0, at document #1000/2819\n",
      "2019-03-04 01:54:26,369 : INFO : merging changes from 1000 documents into a model of 2819 documents\n",
      "2019-03-04 01:54:26,372 : INFO : topic #0 (0.200): 0.006*\"com\" + 0.006*\"like\" + 0.006*\"space\" + 0.004*\"univers\" + 0.004*\"know\" + 0.004*\"nntp\" + 0.004*\"imag\" + 0.004*\"nasa\" + 0.004*\"time\" + 0.004*\"program\"\n",
      "2019-03-04 01:54:26,372 : INFO : topic #1 (0.200): 0.007*\"imag\" + 0.005*\"com\" + 0.004*\"peopl\" + 0.004*\"know\" + 0.004*\"univers\" + 0.004*\"nntp\" + 0.004*\"host\" + 0.004*\"jpeg\" + 0.004*\"good\" + 0.004*\"work\"\n",
      "2019-03-04 01:54:26,373 : INFO : topic #2 (0.200): 0.007*\"peopl\" + 0.005*\"com\" + 0.005*\"like\" + 0.004*\"new\" + 0.004*\"know\" + 0.004*\"think\" + 0.004*\"nntp\" + 0.003*\"isra\" + 0.003*\"host\" + 0.003*\"question\"\n",
      "2019-03-04 01:54:26,374 : INFO : topic #3 (0.200): 0.008*\"com\" + 0.006*\"space\" + 0.005*\"peopl\" + 0.005*\"like\" + 0.004*\"israel\" + 0.004*\"isra\" + 0.004*\"nasa\" + 0.004*\"think\" + 0.003*\"time\" + 0.003*\"right\"\n",
      "2019-03-04 01:54:26,375 : INFO : topic #4 (0.200): 0.006*\"com\" + 0.006*\"time\" + 0.005*\"host\" + 0.005*\"univers\" + 0.005*\"peopl\" + 0.005*\"nntp\" + 0.004*\"know\" + 0.004*\"god\" + 0.004*\"like\" + 0.004*\"think\"\n",
      "2019-03-04 01:54:26,375 : INFO : topic diff=1.680969, rho=1.000000\n",
      "2019-03-04 01:54:26,377 : INFO : PROGRESS: pass 0, at document #2000/2819\n",
      "2019-03-04 01:54:26,883 : INFO : merging changes from 1000 documents into a model of 2819 documents\n",
      "2019-03-04 01:54:26,885 : INFO : topic #0 (0.200): 0.008*\"space\" + 0.007*\"com\" + 0.005*\"imag\" + 0.005*\"like\" + 0.005*\"nasa\" + 0.005*\"program\" + 0.004*\"univers\" + 0.004*\"graphic\" + 0.004*\"data\" + 0.004*\"know\"\n",
      "2019-03-04 01:54:26,886 : INFO : topic #1 (0.200): 0.009*\"imag\" + 0.006*\"com\" + 0.005*\"know\" + 0.004*\"univers\" + 0.004*\"peopl\" + 0.004*\"like\" + 0.004*\"work\" + 0.004*\"want\" + 0.004*\"armenian\" + 0.004*\"think\"\n",
      "2019-03-04 01:54:26,887 : INFO : topic #2 (0.200): 0.012*\"peopl\" + 0.007*\"know\" + 0.006*\"armenian\" + 0.006*\"said\" + 0.006*\"like\" + 0.005*\"think\" + 0.004*\"right\" + 0.004*\"time\" + 0.004*\"wai\" + 0.004*\"thing\"\n",
      "2019-03-04 01:54:26,887 : INFO : topic #3 (0.200): 0.008*\"com\" + 0.007*\"space\" + 0.006*\"israel\" + 0.005*\"peopl\" + 0.005*\"like\" + 0.005*\"isra\" + 0.004*\"nasa\" + 0.004*\"time\" + 0.003*\"think\" + 0.003*\"right\"\n",
      "2019-03-04 01:54:26,888 : INFO : topic #4 (0.200): 0.008*\"god\" + 0.006*\"univers\" + 0.006*\"armenian\" + 0.005*\"com\" + 0.005*\"time\" + 0.005*\"peopl\" + 0.005*\"host\" + 0.004*\"know\" + 0.004*\"nntp\" + 0.004*\"like\"\n",
      "2019-03-04 01:54:26,889 : INFO : topic diff=0.838420, rho=0.707107\n",
      "2019-03-04 01:54:27,485 : INFO : -8.089 per-word bound, 272.3 perplexity estimate based on a held-out corpus of 819 documents with 113268 words\n",
      "2019-03-04 01:54:27,486 : INFO : PROGRESS: pass 0, at document #2819/2819\n",
      "2019-03-04 01:54:27,879 : INFO : merging changes from 819 documents into a model of 2819 documents\n",
      "2019-03-04 01:54:27,882 : INFO : topic #0 (0.200): 0.009*\"space\" + 0.008*\"com\" + 0.006*\"graphic\" + 0.005*\"program\" + 0.005*\"imag\" + 0.004*\"nasa\" + 0.004*\"like\" + 0.004*\"univers\" + 0.004*\"file\" + 0.004*\"new\"\n",
      "2019-03-04 01:54:27,882 : INFO : topic #1 (0.200): 0.007*\"com\" + 0.007*\"imag\" + 0.005*\"univers\" + 0.005*\"know\" + 0.004*\"work\" + 0.004*\"like\" + 0.004*\"host\" + 0.004*\"think\" + 0.004*\"nntp\" + 0.004*\"moral\"\n",
      "2019-03-04 01:54:27,883 : INFO : topic #2 (0.200): 0.011*\"peopl\" + 0.007*\"armenian\" + 0.006*\"said\" + 0.006*\"turkish\" + 0.006*\"know\" + 0.005*\"think\" + 0.005*\"like\" + 0.005*\"right\" + 0.005*\"jew\" + 0.004*\"isra\"\n",
      "2019-03-04 01:54:27,883 : INFO : topic #3 (0.200): 0.008*\"com\" + 0.008*\"israel\" + 0.006*\"space\" + 0.005*\"bike\" + 0.005*\"isra\" + 0.005*\"like\" + 0.004*\"peopl\" + 0.004*\"year\" + 0.004*\"jew\" + 0.004*\"time\"\n",
      "2019-03-04 01:54:27,884 : INFO : topic #4 (0.200): 0.008*\"god\" + 0.007*\"armenian\" + 0.006*\"univers\" + 0.005*\"peopl\" + 0.005*\"com\" + 0.005*\"exist\" + 0.005*\"time\" + 0.004*\"like\" + 0.004*\"think\" + 0.004*\"know\"\n",
      "2019-03-04 01:54:27,885 : INFO : topic diff=0.645193, rho=0.577350\n",
      "2019-03-04 01:54:27,885 : INFO : PROGRESS: pass 1, at document #1000/2819\n",
      "2019-03-04 01:54:28,322 : INFO : merging changes from 1000 documents into a model of 2819 documents\n",
      "2019-03-04 01:54:28,324 : INFO : topic #0 (0.200): 0.010*\"space\" + 0.007*\"com\" + 0.006*\"graphic\" + 0.006*\"nasa\" + 0.005*\"program\" + 0.005*\"imag\" + 0.005*\"file\" + 0.004*\"data\" + 0.004*\"univers\" + 0.004*\"like\"\n",
      "2019-03-04 01:54:28,325 : INFO : topic #1 (0.200): 0.008*\"imag\" + 0.008*\"com\" + 0.005*\"univers\" + 0.005*\"know\" + 0.005*\"like\" + 0.005*\"host\" + 0.005*\"bit\" + 0.005*\"nntp\" + 0.004*\"work\" + 0.004*\"think\"\n",
      "2019-03-04 01:54:28,325 : INFO : topic #2 (0.200): 0.011*\"peopl\" + 0.007*\"armenian\" + 0.006*\"said\" + 0.006*\"right\" + 0.006*\"turkish\" + 0.005*\"know\" + 0.005*\"isra\" + 0.005*\"think\" + 0.005*\"like\" + 0.005*\"jew\"\n",
      "2019-03-04 01:54:28,326 : INFO : topic #3 (0.200): 0.009*\"com\" + 0.008*\"israel\" + 0.006*\"bike\" + 0.006*\"isra\" + 0.005*\"space\" + 0.005*\"like\" + 0.004*\"peopl\" + 0.004*\"year\" + 0.004*\"think\" + 0.004*\"time\"\n",
      "2019-03-04 01:54:28,326 : INFO : topic #4 (0.200): 0.010*\"god\" + 0.007*\"armenian\" + 0.006*\"peopl\" + 0.005*\"univers\" + 0.005*\"com\" + 0.005*\"time\" + 0.005*\"exist\" + 0.004*\"islam\" + 0.004*\"believ\" + 0.004*\"think\"\n",
      "2019-03-04 01:54:28,327 : INFO : topic diff=0.427516, rho=0.455535\n",
      "2019-03-04 01:54:28,327 : INFO : PROGRESS: pass 1, at document #2000/2819\n",
      "2019-03-04 01:54:28,751 : INFO : merging changes from 1000 documents into a model of 2819 documents\n",
      "2019-03-04 01:54:28,753 : INFO : topic #0 (0.200): 0.011*\"space\" + 0.007*\"imag\" + 0.007*\"com\" + 0.006*\"nasa\" + 0.006*\"program\" + 0.005*\"graphic\" + 0.005*\"data\" + 0.004*\"file\" + 0.004*\"univers\" + 0.004*\"new\"\n",
      "2019-03-04 01:54:28,754 : INFO : topic #1 (0.200): 0.009*\"com\" + 0.007*\"imag\" + 0.005*\"like\" + 0.005*\"know\" + 0.005*\"univers\" + 0.005*\"host\" + 0.005*\"nntp\" + 0.004*\"think\" + 0.004*\"moral\" + 0.004*\"work\"\n",
      "2019-03-04 01:54:28,755 : INFO : topic #2 (0.200): 0.013*\"peopl\" + 0.009*\"armenian\" + 0.007*\"said\" + 0.007*\"know\" + 0.006*\"right\" + 0.005*\"like\" + 0.005*\"think\" + 0.005*\"kill\" + 0.005*\"turkish\" + 0.004*\"jew\"\n",
      "2019-03-04 01:54:28,755 : INFO : topic #3 (0.200): 0.009*\"com\" + 0.009*\"israel\" + 0.006*\"isra\" + 0.006*\"bike\" + 0.005*\"like\" + 0.005*\"space\" + 0.004*\"state\" + 0.004*\"think\" + 0.004*\"peopl\" + 0.004*\"host\"\n",
      "2019-03-04 01:54:28,756 : INFO : topic #4 (0.200): 0.010*\"god\" + 0.007*\"armenian\" + 0.006*\"peopl\" + 0.006*\"univers\" + 0.005*\"exist\" + 0.005*\"islam\" + 0.005*\"com\" + 0.005*\"believ\" + 0.005*\"time\" + 0.004*\"think\"\n",
      "2019-03-04 01:54:28,756 : INFO : topic diff=0.427394, rho=0.455535\n",
      "2019-03-04 01:54:29,297 : INFO : -7.871 per-word bound, 234.1 perplexity estimate based on a held-out corpus of 819 documents with 113268 words\n",
      "2019-03-04 01:54:29,298 : INFO : PROGRESS: pass 1, at document #2819/2819\n",
      "2019-03-04 01:54:29,633 : INFO : merging changes from 819 documents into a model of 2819 documents\n",
      "2019-03-04 01:54:29,635 : INFO : topic #0 (0.200): 0.011*\"space\" + 0.007*\"com\" + 0.006*\"graphic\" + 0.006*\"imag\" + 0.006*\"program\" + 0.005*\"nasa\" + 0.004*\"file\" + 0.004*\"launch\" + 0.004*\"univers\" + 0.004*\"new\"\n",
      "2019-03-04 01:54:29,636 : INFO : topic #1 (0.200): 0.010*\"com\" + 0.006*\"like\" + 0.006*\"univers\" + 0.005*\"imag\" + 0.005*\"know\" + 0.005*\"host\" + 0.005*\"nntp\" + 0.005*\"think\" + 0.005*\"work\" + 0.004*\"henri\"\n",
      "2019-03-04 01:54:29,637 : INFO : topic #2 (0.200): 0.012*\"peopl\" + 0.009*\"armenian\" + 0.008*\"turkish\" + 0.007*\"said\" + 0.006*\"jew\" + 0.006*\"right\" + 0.006*\"know\" + 0.005*\"kill\" + 0.005*\"like\" + 0.005*\"isra\"\n",
      "2019-03-04 01:54:29,637 : INFO : topic #3 (0.200): 0.010*\"com\" + 0.009*\"israel\" + 0.008*\"bike\" + 0.006*\"isra\" + 0.005*\"like\" + 0.005*\"dod\" + 0.004*\"motorcycl\" + 0.004*\"think\" + 0.004*\"state\" + 0.004*\"year\"\n",
      "2019-03-04 01:54:29,638 : INFO : topic #4 (0.200): 0.010*\"god\" + 0.007*\"armenian\" + 0.006*\"peopl\" + 0.006*\"exist\" + 0.006*\"univers\" + 0.005*\"com\" + 0.005*\"think\" + 0.005*\"believ\" + 0.004*\"islam\" + 0.004*\"like\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-04 01:54:29,638 : INFO : topic diff=0.416347, rho=0.455535\n",
      "2019-03-04 01:54:29,639 : INFO : PROGRESS: pass 2, at document #1000/2819\n",
      "2019-03-04 01:54:30,031 : INFO : merging changes from 1000 documents into a model of 2819 documents\n",
      "2019-03-04 01:54:30,034 : INFO : topic #0 (0.200): 0.011*\"space\" + 0.007*\"imag\" + 0.006*\"nasa\" + 0.006*\"com\" + 0.006*\"graphic\" + 0.006*\"program\" + 0.005*\"file\" + 0.005*\"data\" + 0.005*\"orbit\" + 0.004*\"new\"\n",
      "2019-03-04 01:54:30,034 : INFO : topic #1 (0.200): 0.010*\"com\" + 0.006*\"imag\" + 0.006*\"like\" + 0.006*\"nntp\" + 0.006*\"host\" + 0.006*\"univers\" + 0.005*\"bit\" + 0.005*\"know\" + 0.005*\"think\" + 0.005*\"work\"\n",
      "2019-03-04 01:54:30,035 : INFO : topic #2 (0.200): 0.011*\"peopl\" + 0.009*\"armenian\" + 0.007*\"turkish\" + 0.007*\"said\" + 0.007*\"right\" + 0.006*\"jew\" + 0.005*\"know\" + 0.005*\"kill\" + 0.005*\"isra\" + 0.005*\"like\"\n",
      "2019-03-04 01:54:30,036 : INFO : topic #3 (0.200): 0.010*\"com\" + 0.010*\"israel\" + 0.008*\"bike\" + 0.006*\"isra\" + 0.005*\"like\" + 0.004*\"dod\" + 0.004*\"host\" + 0.004*\"think\" + 0.004*\"nntp\" + 0.004*\"state\"\n",
      "2019-03-04 01:54:30,037 : INFO : topic #4 (0.200): 0.011*\"god\" + 0.007*\"peopl\" + 0.006*\"armenian\" + 0.006*\"exist\" + 0.005*\"univers\" + 0.005*\"islam\" + 0.005*\"com\" + 0.005*\"think\" + 0.005*\"believ\" + 0.005*\"atheist\"\n",
      "2019-03-04 01:54:30,037 : INFO : topic diff=0.332566, rho=0.414549\n",
      "2019-03-04 01:54:30,037 : INFO : PROGRESS: pass 2, at document #2000/2819\n",
      "2019-03-04 01:54:30,419 : INFO : merging changes from 1000 documents into a model of 2819 documents\n",
      "2019-03-04 01:54:30,421 : INFO : topic #0 (0.200): 0.012*\"space\" + 0.008*\"imag\" + 0.007*\"nasa\" + 0.006*\"com\" + 0.006*\"program\" + 0.006*\"graphic\" + 0.005*\"data\" + 0.005*\"file\" + 0.004*\"new\" + 0.004*\"univers\"\n",
      "2019-03-04 01:54:30,422 : INFO : topic #1 (0.200): 0.011*\"com\" + 0.007*\"like\" + 0.006*\"know\" + 0.006*\"nntp\" + 0.006*\"host\" + 0.006*\"univers\" + 0.005*\"imag\" + 0.005*\"bit\" + 0.005*\"think\" + 0.005*\"henri\"\n",
      "2019-03-04 01:54:30,423 : INFO : topic #2 (0.200): 0.013*\"peopl\" + 0.010*\"armenian\" + 0.008*\"said\" + 0.007*\"know\" + 0.006*\"right\" + 0.006*\"turkish\" + 0.006*\"kill\" + 0.006*\"jew\" + 0.005*\"like\" + 0.005*\"think\"\n",
      "2019-03-04 01:54:30,424 : INFO : topic #3 (0.200): 0.010*\"com\" + 0.009*\"israel\" + 0.008*\"bike\" + 0.007*\"isra\" + 0.005*\"like\" + 0.005*\"host\" + 0.004*\"nntp\" + 0.004*\"state\" + 0.004*\"think\" + 0.004*\"dod\"\n",
      "2019-03-04 01:54:30,424 : INFO : topic #4 (0.200): 0.011*\"god\" + 0.007*\"peopl\" + 0.006*\"exist\" + 0.006*\"univers\" + 0.005*\"armenian\" + 0.005*\"islam\" + 0.005*\"believ\" + 0.005*\"think\" + 0.005*\"com\" + 0.005*\"atheist\"\n",
      "2019-03-04 01:54:30,425 : INFO : topic diff=0.326337, rho=0.414549\n",
      "2019-03-04 01:54:30,943 : INFO : -7.813 per-word bound, 224.8 perplexity estimate based on a held-out corpus of 819 documents with 113268 words\n",
      "2019-03-04 01:54:30,943 : INFO : PROGRESS: pass 2, at document #2819/2819\n",
      "2019-03-04 01:54:31,250 : INFO : merging changes from 819 documents into a model of 2819 documents\n",
      "2019-03-04 01:54:31,252 : INFO : topic #0 (0.200): 0.012*\"space\" + 0.007*\"imag\" + 0.006*\"graphic\" + 0.006*\"nasa\" + 0.006*\"program\" + 0.006*\"com\" + 0.005*\"launch\" + 0.005*\"file\" + 0.004*\"new\" + 0.004*\"univers\"\n",
      "2019-03-04 01:54:31,253 : INFO : topic #1 (0.200): 0.012*\"com\" + 0.006*\"like\" + 0.006*\"nntp\" + 0.006*\"host\" + 0.006*\"univers\" + 0.006*\"know\" + 0.005*\"think\" + 0.005*\"work\" + 0.005*\"henri\" + 0.005*\"bit\"\n",
      "2019-03-04 01:54:31,254 : INFO : topic #2 (0.200): 0.012*\"peopl\" + 0.011*\"armenian\" + 0.008*\"turkish\" + 0.007*\"said\" + 0.007*\"jew\" + 0.006*\"right\" + 0.006*\"know\" + 0.005*\"kill\" + 0.005*\"isra\" + 0.005*\"like\"\n",
      "2019-03-04 01:54:31,254 : INFO : topic #3 (0.200): 0.011*\"com\" + 0.010*\"israel\" + 0.009*\"bike\" + 0.006*\"isra\" + 0.006*\"dod\" + 0.005*\"like\" + 0.005*\"motorcycl\" + 0.004*\"host\" + 0.004*\"ride\" + 0.004*\"think\"\n",
      "2019-03-04 01:54:31,255 : INFO : topic #4 (0.200): 0.010*\"god\" + 0.007*\"peopl\" + 0.006*\"exist\" + 0.006*\"univers\" + 0.005*\"think\" + 0.005*\"armenian\" + 0.005*\"com\" + 0.005*\"believ\" + 0.005*\"islam\" + 0.005*\"christian\"\n",
      "2019-03-04 01:54:31,256 : INFO : topic diff=0.317634, rho=0.414549\n",
      "2019-03-04 01:54:31,256 : INFO : PROGRESS: pass 3, at document #1000/2819\n"
     ]
    }
   ],
   "source": [
    "tm_metrics = pd.DataFrame(columns=['model', 'train_time', 'coherence', 'l2_norm', 'f1', 'topics'])\n",
    "\n",
    "y_train = [doc['target'] for doc in trainset]\n",
    "y_test = [doc['target'] for doc in testset]\n",
    "\n",
    "# LDA metrics\n",
    "row = {}\n",
    "row['model'] = 'lda'\n",
    "row['train_time'], row['mean_ram'], row['max_ram'], lda = get_train_time_and_ram(\n",
    "    lambda: LdaModel(\n",
    "        corpus=train_corpus,\n",
    "        **fixed_params,\n",
    "    ),\n",
    "    'lda',\n",
    "    1,\n",
    ")\n",
    "row.update(get_metrics(\n",
    "    lda, test_corpus, train_corpus, y_train, y_test,\n",
    "))\n",
    "tm_metrics = tm_metrics.append(pd.Series(row), ignore_index=True)\n",
    "\n",
    "# Sklearn NMF metrics\n",
    "row = {}\n",
    "row['model'] = 'sklearn_nmf'\n",
    "train_dense_corpus_tfidf = matutils.corpus2dense(train_corpus_tfidf, len(dictionary)).T\n",
    "row['train_time'], row['mean_ram'], row['max_ram'], sklearn_nmf = get_train_time_and_ram(\n",
    "    lambda: SklearnNmf(n_components=5, random_state=42).fit(train_dense_corpus_tfidf),\n",
    "    'sklearn_nmf',\n",
    "    1,\n",
    ")\n",
    "row.update(get_metrics(\n",
    "    sklearn_nmf, test_corpus_tfidf, train_corpus_tfidf, y_train, y_test, dictionary,\n",
    "))\n",
    "tm_metrics = tm_metrics.append(pd.Series(row), ignore_index=True)\n",
    "\n",
    "# Gensim NMF metrics\n",
    "row = {}\n",
    "row['model'] = 'gensim_nmf'\n",
    "row['train_time'], row['mean_ram'], row['max_ram'], gensim_nmf = get_train_time_and_ram(\n",
    "    lambda: GensimNmf(\n",
    "        normalize=False,\n",
    "        corpus=train_corpus_tfidf,\n",
    "        **fixed_params\n",
    "    ),\n",
    "    'gensim_nmf',\n",
    "    0.5,\n",
    ")\n",
    "row.update(get_metrics(\n",
    "    gensim_nmf, test_corpus_tfidf, train_corpus_tfidf, y_train, y_test,\n",
    "))\n",
    "tm_metrics = tm_metrics.append(pd.Series(row), ignore_index=True)\n",
    "tm_metrics.replace(np.nan, '-', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_metrics.drop('topics', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main insights\n",
    "\n",
    "- Gensim NMF is **ridiculously fast** and leaves both LDA and Sklearn far behind in terms of training time and quality on downstream task (F1 score), though coherence is the lowest among all models.\n",
    "- Gensim NMF beats Sklearn NMF in RAM consumption, but L2 norm is a bit worse.\n",
    "- Gensim NMF consumes a bit more RAM than LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learned topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the 5 topics learned by each of the three models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_topics(tm_metrics):\n",
    "    for _, row in tm_metrics.iterrows():\n",
    "        print('\\n{}:'.format(row.model))\n",
    "        print(\"\\n\".join(str(topic) for topic in row.topics))\n",
    "        \n",
    "compare_topics(tm_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subjectively, Gensim and Sklearn NMFs are on par with each other, LDA looks a bit worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. NMF on English Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section shows how to train an NMF model on a large text corpus, the entire English Wikipedia: **2.6 billion words, in 23.1 million article sections across 5 million Wikipedia articles**.\n",
    "\n",
    "The data preprocessing takes a while, and we'll be comparing multiple models, so **reserve about FIXME hours** and some **20 GB of disk space** to go through the following notebook cells in full. You'll need `gensim>=3.7.1`, `numpy`, `tqdm`, `pandas`, `psutils`, `joblib` and `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import modules from scratch, so that this Section doesn't rely on any previous cells.\n",
    "import itertools\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "import os\n",
    "\n",
    "from smart_open import smart_open\n",
    "import psutil\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "from contextlib import contextmanager, contextmanager, contextmanager\n",
    "from multiprocessing import Process\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.decomposition.nmf import NMF as SklearnNmf\n",
    "\n",
    "import gensim.downloader\n",
    "from gensim import matutils\n",
    "from gensim.corpora import MmCorpus, Dictionary\n",
    "from gensim.models import LdaModel, LdaMulticore, CoherenceModel\n",
    "from gensim.models.nmf import Nmf as GensimNmf\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Wikipedia dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the [gensim.downloader](https://github.com/RaRe-Technologies/gensim-data) to download a parsed Wikipedia dump (6.1 GB disk space):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = gensim.downloader.load(\"wiki-english-20171001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the titles and sections of the first Wikipedia article, as a little sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = gensim.downloader.load(\"wiki-english-20171001\")\n",
    "article = next(iter(data))\n",
    "\n",
    "print(\"Article: %r\\n\" % article['title'])\n",
    "for section_title, section_text in zip(article['section_titles'], article['section_texts']):\n",
    "    print(\"Section title: %r\" % section_title)\n",
    "    print(\"Section text: %sâ€¦\\n\" % section_text[:100].replace('\\n', ' ').strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a Python generator function that streams through the downloaded Wikipedia dump and preprocesses (tokenizes, lower-cases) each article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikidump2tokens(articles):\n",
    "    \"\"\"Stream through the Wikipedia dump, yielding a list of tokens for each article.\"\"\"\n",
    "    for article in articles:\n",
    "        article_section_texts = [\n",
    "            \" \".join([title, text])\n",
    "            for title, text\n",
    "            in zip(article['section_titles'], article['section_texts'])\n",
    "        ]\n",
    "        article_tokens = simple_preprocess(\" \".join(article_section_texts))\n",
    "        yield article_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a word-to-id mapping, in order to vectorize texts. Makes a full pass over the Wikipedia corpus, takes **~3.5 hours**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists('wiki.dict'):\n",
    "    # If we already stored the Dictionary in a previous run, simply load it, to save time.\n",
    "    dictionary = Dictionary.load('wiki.dict')\n",
    "else:\n",
    "    dictionary = Dictionary(wikidump2tokens(data))\n",
    "    # Keep only the 30,000 most frequent vocabulary terms, after filtering away terms\n",
    "    # that are too frequent/too infrequent.\n",
    "    dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=30000)\n",
    "    dictionary.save('wiki.dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store preprocessed Wikipedia as bag-of-words sparse matrix in MatrixMarket format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training NMF with a single pass over the input corpus (\"online\"), we simply vectorize each raw text straight from the input storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_stream = (dictionary.doc2bow(article) for article in wikidump2tokens(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this tutorial though, we'll serialize (\"cache\") the vectorized bag-of-words vectors to disk, to `wiki.mm` file in MatrixMarket format. The reason is, we'll be re-using the vectorized articles multiple times, for different models for our benchmarks, and also shuffling them, so it makes sense to amortize the vectorization time by persisting the resulting vectors to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's stream through the preprocessed sparse Wikipedia bag-of-words matrix while storing it to disk. **This step takes about 3 hours** and needs **38 GB of disk space**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomSplitCorpus(MmCorpus):\n",
    "    \"\"\"\n",
    "    Use the fact that MmCorpus supports random indexing, and create a streamed\n",
    "    corpus in shuffled order, including a train/test split for evaluation.\n",
    "    \"\"\"\n",
    "    def __init__(self, random_seed=42, testset=False, testsize=1000, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        random_state = np.random.RandomState(random_seed)\n",
    "        \n",
    "        self.indices = random_state.permutation(range(self.num_docs))\n",
    "        test_nnz = sum(len(self[doc_idx]) for doc_idx in self.indices[:testsize])\n",
    "        \n",
    "        if testset:\n",
    "            self.indices = self.indices[:testsize]\n",
    "            self.num_docs = testsize\n",
    "            self.num_nnz = test_nnz\n",
    "        else:\n",
    "            self.indices = self.indices[testsize:]\n",
    "            self.num_docs -= testsize\n",
    "            self.num_nnz -= test_nnz\n",
    "\n",
    "    def __iter__(self):\n",
    "        for doc_id in self.indices:\n",
    "            yield self[doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('wiki.mm'):\n",
    "    MmCorpus.serialize('wiki.mm', vector_stream, progress_cnt=100000)\n",
    "\n",
    "if not os.path.exists('wiki_tfidf.mm'):\n",
    "    MmCorpus.serialize('wiki_tfidf.mm', tfidf[MmCorpus('wiki.mm')], progress_cnt=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load back the vectors as two lazily-streamed train/test iterables.\n",
    "train_corpus = RandomSplitCorpus(\n",
    "    random_seed=42, testset=False, testsize=10000, fname='wiki.mm',\n",
    ")\n",
    "test_corpus = RandomSplitCorpus(\n",
    "    random_seed=42, testset=True, testsize=10000, fname='wiki.mm',\n",
    ")\n",
    "\n",
    "train_corpus_tfidf = RandomSplitCorpus(\n",
    "    random_seed=42, testset=False, testsize=10000, fname='wiki_tfidf.mm',\n",
    ")\n",
    "test_corpus_tfidf = RandomSplitCorpus(\n",
    "    random_seed=42, testset=True, testsize=10000, fname='wiki_tfidf.mm',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save preprocessed Wikipedia in scipy.sparse format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is only needed to run the Sklearn NMF on Wikipedia, for comparison in the benchmarks below. Sklearn expects in-memory scipy sparse input, not on-the-fly vector streams. Needs additional ~2 GB of disk space.\n",
    "\n",
    "\n",
    "**Skip this step if you don't need the Sklearn's NMF benchmark, and only want to run Gensim's NMF.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('wiki_train_csr.npz'):\n",
    "    scipy.sparse.save_npz(\n",
    "        'wiki_train_csr.npz',\n",
    "        matutils.corpus2csc(train_corpus_tfidf, len(dictionary)).T,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "We'll track these metrics as we train and test NMF on the Wikipedia corpus we created above:\n",
    "- `train time` - time to train a model\n",
    "- `mean_ram` - mean RAM consumption during training\n",
    "- `max_ram` - maximum RAM consumption during training\n",
    "- `train time` - time to train a model.\n",
    "- `coherence` - coherence score.\n",
    "- `l2_norm` - L2 norm of `v - Wh` (not defined for LDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a dataframe in which we'll store the recorded metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_metrics = pd.DataFrame(columns=[\n",
    "    'model', 'train_time', 'mean_ram', 'max_ram', 'coherence', 'l2_norm', 'topics',\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define common parameters, to be shared by all evaluated models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(\n",
    "    chunksize=2000,\n",
    "    num_topics=50,\n",
    "    id2word=dictionary,\n",
    "    passes=1,\n",
    "    eval_every=10,\n",
    "    minimum_probability=0,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Gensim NMF model and record its metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "row = {}\n",
    "row['model'] = 'gensim_nmf'\n",
    "row['train_time'], row['mean_ram'], row['max_ram'], nmf = get_train_time_and_ram(\n",
    "    lambda: GensimNmf(normalize=False, corpus=train_corpus_tfidf, **params),\n",
    "    'gensim_nmf',\n",
    "    1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(row)\n",
    "nmf.save('gensim_nmf.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = GensimNmf.load('gensim_nmf.model')\n",
    "row.update(get_metrics(nmf, test_corpus_tfidf))\n",
    "print(row)\n",
    "tm_metrics = tm_metrics.append(pd.Series(row), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Gensim LDA and record its metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "row = {}\n",
    "row['model'] = 'lda'\n",
    "row['train_time'], row['mean_ram'], row['max_ram'], lda = get_train_time_and_ram(\n",
    "    lambda: LdaModel(corpus=train_corpus, **params),\n",
    "    'lda',\n",
    "    1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(row)\n",
    "lda.save('lda.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaModel.load('lda.model')\n",
    "row.update(get_metrics(lda, test_corpus))\n",
    "print(row)\n",
    "tm_metrics = tm_metrics.append(pd.Series(row), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Sklearn NMF and record its metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Careful!** Sklearn loads the entire input Wikipedia matrix into RAM. Even though the matrix is sparse, **you'll need FIXME GB of free RAM to run the cell below**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = {}\n",
    "row['model'] = 'sklearn_nmf'\n",
    "sklearn_nmf = SklearnNmf(n_components=50, tol=1e-2, random_state=42)\n",
    "row['train_time'], row['mean_ram'], row['max_ram'], sklearn_nmf = get_train_time_and_ram(\n",
    "    lambda: sklearn_nmf.fit(scipy.sparse.load_npz('wiki_train_csr.npz')),\n",
    "    'sklearn_nmf',\n",
    "    10,\n",
    ")\n",
    "print(row)\n",
    "joblib.dump(sklearn_nmf, 'sklearn_nmf.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_nmf = joblib.load('sklearn_nmf.joblib')\n",
    "row.update(get_metrics(\n",
    "    sklearn_nmf, test_corpus_tfidf, dictionary=dictionary,\n",
    "))\n",
    "print(row)\n",
    "tm_metrics = tm_metrics.append(pd.Series(row), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_metrics.replace(np.nan, '-', inplace=True)\n",
    "tm_metrics.drop(['topics', 'f1'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights\n",
    "\n",
    "Gensim's online NMF outperforms Sklearn's NMF in every aspect on this dataset:\n",
    "\n",
    "- **2x** faster.\n",
    "\n",
    "- Uses **~100x** less memory.\n",
    "\n",
    "    About **8GB** of Sklearn's RAM comes from the in-memory input matrices, which, in contrast to Gensim NMF, cannot be streamed iteratively. But even if we forget about the huge input size, Sklearn NMF uses about **2-8 GB** of RAM â€“ significantly more than Gensim NMF or LDA.\n",
    "\n",
    "- Achieves better L2 norm and perplexity.\n",
    "\n",
    "Compared to Gensim's LDA, Gensim NMF also gives superior results:\n",
    "\n",
    "- **3.5x** faster\n",
    "- Achieves much better L2 norm and perplexity\n",
    "- Coherence is worse than LDA's though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learned Wikipedia topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_topics(tm_metrics):\n",
    "    for _, row in tm_metrics.iterrows():\n",
    "        print('\\n{}:'.format(row.model))\n",
    "        print(\"\\n\".join(str(topic) for topic in row.topics))\n",
    "        \n",
    "compare_topics(tm_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems all three models successfully learned useful topics from the Wikipedia corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. And now for something completely different: Face decomposition from images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NMF algorithm in Gensim is optimized for extremely large (sparse) text corpora, but it will also work on vectors from other domains!\n",
    "\n",
    "Let's compare our model to other factorization algorithms on dense image vectors and check out the results.\n",
    "\n",
    "To do that we'll patch sklearn's [Faces Dataset Decomposition](https://scikit-learn.org/stable/auto_examples/decomposition/plot_faces_decomposition.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn wrapper\n",
    "Let's create an Scikit-learn wrapper in order to run Gensim NMF on images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from numpy.random import RandomState\n",
    "from sklearn import decomposition\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.decomposition.nmf import NMF as SklearnNmf\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "import gensim.downloader\n",
    "from gensim import matutils\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel, LdaModel, LdaMulticore\n",
    "from gensim.models.nmf import Nmf as GensimNmf\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "\n",
    "class NmfWrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, bow_matrix, **kwargs):\n",
    "        self.corpus = sparse.csc.csc_matrix(bow_matrix)\n",
    "        self.nmf = GensimNmf(**kwargs)\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.nmf.update(self.corpus)\n",
    "\n",
    "    @property\n",
    "    def components_(self):\n",
    "        return self.nmf.get_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified face decomposition notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from the excellent [Scikit-learn tutorial](https://github.com/scikit-learn/scikit-learn/blob/master/examples/decomposition/plot_faces_decomposition.py) (BSD license):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn off the logger due to large number of info messages during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim.models.nmf.logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "============================\n",
    "Faces dataset decompositions\n",
    "============================\n",
    "\n",
    "This example applies to :ref:`olivetti_faces` different unsupervised\n",
    "matrix decomposition (dimension reduction) methods from the module\n",
    ":py:mod:`sklearn.decomposition` (see the documentation chapter\n",
    ":ref:`decompositions`) .\n",
    "\n",
    "\"\"\"\n",
    "print(__doc__)\n",
    "\n",
    "# Authors: Vlad Niculae, Alexandre Gramfort\n",
    "# License: BSD 3 claus\n",
    "\n",
    "n_row, n_col = 2, 3\n",
    "n_components = n_row * n_col\n",
    "image_shape = (64, 64)\n",
    "rng = RandomState(0)\n",
    "\n",
    "# #############################################################################\n",
    "# Load faces data\n",
    "dataset = fetch_olivetti_faces(shuffle=True, random_state=rng)\n",
    "faces = dataset.data\n",
    "\n",
    "n_samples, n_features = faces.shape\n",
    "\n",
    "# global centering\n",
    "faces_centered = faces - faces.mean(axis=0)\n",
    "\n",
    "# local centering\n",
    "faces_centered -= faces_centered.mean(axis=1).reshape(n_samples, -1)\n",
    "\n",
    "print(\"Dataset consists of %d faces\" % n_samples)\n",
    "\n",
    "\n",
    "def plot_gallery(title, images, n_col=n_col, n_row=n_row):\n",
    "    plt.figure(figsize=(2. * n_col, 2.26 * n_row))\n",
    "    plt.suptitle(title, size=16)\n",
    "    for i, comp in enumerate(images):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        vmax = max(comp.max(), -comp.min())\n",
    "        plt.imshow(comp.reshape(image_shape), cmap=plt.cm.gray,\n",
    "                   interpolation='nearest',\n",
    "                   vmin=-vmax, vmax=vmax)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "    plt.subplots_adjust(0.01, 0.05, 0.99, 0.93, 0.04, 0.)\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# List of the different estimators, whether to center and transpose the\n",
    "# problem, and whether the transformer uses the clustering API.\n",
    "estimators = [\n",
    "    ('Eigenfaces - PCA using randomized SVD',\n",
    "     decomposition.PCA(n_components=n_components, svd_solver='randomized',\n",
    "                       whiten=True),\n",
    "     True),\n",
    "\n",
    "    ('Non-negative components - NMF (Sklearn)',\n",
    "     decomposition.NMF(n_components=n_components, init='nndsvda', tol=5e-3),\n",
    "     False),\n",
    "\n",
    "    ('Non-negative components - NMF (Gensim)',\n",
    "     NmfWrapper(\n",
    "         bow_matrix=faces.T,\n",
    "         chunksize=3,\n",
    "         eval_every=400,\n",
    "         passes=2,\n",
    "         id2word={idx: idx for idx in range(faces.shape[1])},\n",
    "         num_topics=n_components,\n",
    "         minimum_probability=0,\n",
    "         random_state=42,\n",
    "     ),\n",
    "     False),\n",
    "\n",
    "    ('Independent components - FastICA',\n",
    "     decomposition.FastICA(n_components=n_components, whiten=True),\n",
    "     True),\n",
    "\n",
    "    ('Sparse comp. - MiniBatchSparsePCA',\n",
    "     decomposition.MiniBatchSparsePCA(n_components=n_components, alpha=0.8,\n",
    "                                      n_iter=100, batch_size=3,\n",
    "                                      random_state=rng),\n",
    "     True),\n",
    "\n",
    "    ('MiniBatchDictionaryLearning',\n",
    "     decomposition.MiniBatchDictionaryLearning(n_components=15, alpha=0.1,\n",
    "                                               n_iter=50, batch_size=3,\n",
    "                                               random_state=rng),\n",
    "     True),\n",
    "\n",
    "    ('Cluster centers - MiniBatchKMeans',\n",
    "     MiniBatchKMeans(n_clusters=n_components, tol=1e-3, batch_size=20,\n",
    "                     max_iter=50, random_state=rng),\n",
    "     True),\n",
    "\n",
    "    ('Factor Analysis components - FA',\n",
    "     decomposition.FactorAnalysis(n_components=n_components, max_iter=2),\n",
    "     True),\n",
    "]\n",
    "\n",
    "# #############################################################################\n",
    "# Plot a sample of the input data\n",
    "\n",
    "plot_gallery(\"First centered Olivetti faces\", faces_centered[:n_components])\n",
    "\n",
    "# #############################################################################\n",
    "# Do the estimation and plot it\n",
    "\n",
    "for name, estimator, center in estimators:\n",
    "    print(\"Extracting the top %d %s...\" % (n_components, name))\n",
    "    t0 = time.time()\n",
    "    data = faces\n",
    "    if center:\n",
    "        data = faces_centered\n",
    "    estimator.fit(data)\n",
    "    train_time = (time.time() - t0)\n",
    "    print(\"done in %0.3fs\" % train_time)\n",
    "    if hasattr(estimator, 'cluster_centers_'):\n",
    "        components_ = estimator.cluster_centers_\n",
    "    else:\n",
    "        components_ = estimator.components_\n",
    "\n",
    "    # Plot an image representing the pixelwise variance provided by the\n",
    "    # estimator e.g its noise_variance_ attribute. The Eigenfaces estimator,\n",
    "    # via the PCA decomposition, also provides a scalar noise_variance_\n",
    "    # (the mean of pixelwise variance) that cannot be displayed as an image\n",
    "    # so we skip it.\n",
    "    if (hasattr(estimator, 'noise_variance_') and\n",
    "            estimator.noise_variance_.ndim > 0):  # Skip the Eigenfaces case\n",
    "        plot_gallery(\"Pixelwise variance\",\n",
    "                     estimator.noise_variance_.reshape(1, -1), n_col=1,\n",
    "                     n_row=1)\n",
    "    plot_gallery('%s - Train time %.1fs' % (name, train_time),\n",
    "                 components_[:n_components])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, Gensim's NMF implementation is slower than Sklearn's on **dense** vectors, while achieving comparable quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Gensim NMF is an extremely fast and memory-optimized model. Use it to obtain interpretable topics, as an alternative to SVD / LDA.\n",
    "\n",
    "---\n",
    "\n",
    "The NMF implementation in Gensim was created by [Timofey Yefimov](https://github.com/anotherbugmaster/) as a part of his [RARE Technologies Student Incubator](https://rare-technologies.com/incubator/) graduation project."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "percent",
    "format_version": "1.1",
    "jupytext_version": "0.8.3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
